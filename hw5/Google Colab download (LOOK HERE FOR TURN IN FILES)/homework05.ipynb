{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4pJohDkIbwu",
    "tags": [
     "T"
    ]
   },
   "source": [
    "> **Student Names and IDs**:\n",
    ">\n",
    "> - Pablo Ortiz, 0686443\n",
    "> - Jongwan Park, 0848815"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93F4Xn6mIbww",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aymIv7MFIbwz",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "## Homework Submission Workflow\n",
    "\n",
    "When you submit your work, follow the instructions on the [submission workflow page](https://www.cs.duke.edu/courses/fall18/compsci371d/homework/workflow.html) for full credit, but see the changes mentioned below.\n",
    "\n",
    "**Important: Failure to do any of the following will result in lost points:**\n",
    "\n",
    "- Submit **one** PDF file and **one** notebook per group\n",
    "\n",
    "- Enter **all the group members** through the Gradescope GUI when you submit your PDF files. It is **not** enough to list group members in your documents\n",
    "\n",
    "- Match each **answer** (not question!) with the appropriate page in Gradescope\n",
    "\n",
    "- Avoid large blank spaces in your PDF file\n",
    "\n",
    "**Important changes to homework preparation workflow:** _This assignment is different from the others in that you are required to run it on the Google Colaboratory, a cloud service that Google makes available for reseach in machine learning. This is necessary because some of the problems require you to train a deep network on hardware that is faster than what is typically available on a standard laptop or desktop, including a high-end GPU. Even if you do have a high-end GPU, please run your notebook on the Colaboratory, so we can grade your work consistently._\n",
    "\n",
    "_**To work on this assignment, go to the [Colaboratory](https://colab.research.google.com) and upload the template notebook for this assignment through the `File` menu at the top of the Colaboratory page. Then work on the assignment, making sure to pay attention to instructions in Part 4 where you are asked to change the runtime type.**_\n",
    "\n",
    "_**When you are done, download the notebook (after making sure that all the outputs from running the code show up properly), and proceed as usual to turn that notebook into a PDF file for submission.**_\n",
    "\n",
    "#### Programming Notes\n",
    "\n",
    "+ The Colaboratory is a cloud service. If a notebook sits idle for a long time, it automatically disconnects from its execution kernel, and you need to rerun all the cells.\n",
    "+ Some of the cells in the Part on neural networks are to be run with different runtime types, as explained later. Because of this, you will not be able to just restart the notebook and run all its cells with a single command. Instead, you need to run the cells one at a time, changing runtime type as instructed. Make sure you do this once you are done with the assignment, making sure that the output from your code matches the text where you describe that output.\n",
    "+ Depending on circumstances, changing the runtime type may erase some or all of the notebook state. This will require you to rerun the cells that generate state.\n",
    "+ Training depends on random initialization of the network parameters. Because of this, your results may vary relative to the sample solution, even if your code is no different. Results may also vary from run to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIp6dzlwIbw0",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "## Part 1: Exam-Style Questions, Set 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6o90sGDiK4aF",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "The small neural net in the figure below uses the ReLU as the nonlinearity at the output of each neuron. The values specified in the hollow circles are biases, and the values along the edges are gains. Weigths number 1, 2, 3 refer to the first neuron, 4, 5,6 to the second, 7, 8, 9 to the third."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CakDYmy0Mk13",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "![a simple neural network](https://www2.cs.duke.edu/courses/spring19/compsci527/homework/5/netSimple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdrtWLQ5JZ7q",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CacoODWaJdmL",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Are all the layers in the net above fully connected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJ9w1-GpN1FM",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6Y_IoC4tUKM"
   },
   "source": [
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4o_0Co16OJ-v",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WmVrDJydN7OP",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "What is the output $y$ from the net above when the input is as follows?\n",
    "\n",
    "$$\n",
    "x_1 = 0 \\;\\;\\; \\text{and}\\;\\;\\; x_2 = 3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-LZGjFMORa8",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pi7jMclStUKW"
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "y &= w_7 ReLU(w_1 x_1 + w_2 x_2 + w_3) + w_8 ReLU(w_4 x_1 + w_5 x_2 + w_6) + w_9\\\\\n",
    "&= -1*ReLU(1*0 + -2*3 + 0) + 1*ReLU(2*0 + 1*3 + 0) + 1 = 0 + 3 + 1 \\\\\n",
    "&= 4.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rP4qW1wUQapl",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2WZXG8wNQeJ5",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "What is the gradient $\\mathbf{g}$ of the output $y$ of the network above with respect to the weight vector\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = [w_1,\\ w_2,\\ w_3,\\ w_4,\\ w_5,\\ w_6,\\ w_7,\\ w_8,\\ w_9]^T\n",
    "$$\n",
    "\n",
    "when the input has the values given in the previous problem? Just give the result if you are confident of your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fVXbN93RDq7",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sjs3IbzrtUKc"
   },
   "source": [
    "Define:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_1 &= w_1 x_1 + w_2 x_2 + w_3 \\\\\n",
    "&= 3w_2 + w_3, \\\\\n",
    "z_2 &= w_4 x_1 + w_5 x_2 + w_6 \\\\\n",
    "&= 3w_5 + w_6. \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial y}{\\partial \\mathbf{w}} &=\n",
    "\\left[\\begin{array}@\n",
    "\\frac{\\partial y}{\\partial w_1}\\;\\;\n",
    "\\frac{\\partial y}{\\partial w_2}\\;\\;\n",
    "\\frac{\\partial y}{\\partial w_3}\\;\\;\n",
    "\\frac{\\partial y}{\\partial w_4}\\;\\;\n",
    "\\frac{\\partial y}{\\partial w_5}\\;\\;\n",
    "\\frac{\\partial y}{\\partial w_6}\\;\\;\n",
    "\\frac{\\partial y}{\\partial w_7}\\;\\;\n",
    "\\frac{\\partial y}{\\partial w_8}\\;\\;\n",
    "\\frac{\\partial y}{\\partial w_9}\n",
    "\\end{array}\\right]\\\\\n",
    "&= \\left[\\begin{array}@\n",
    "0\\;\\;\n",
    "3 w_7 H(z_1)\\;\\;\n",
    "w_7 H(z_1)\\;\\;\n",
    "0\\;\\;\n",
    "3 w_8 H(z_2)\\;\\;\n",
    "w_8 H(z_2)\\;\\;\n",
    "ReLU(z_1)\\;\\;\n",
    "ReLU(z_2)\\;\\;\n",
    "1\n",
    "\\end{array}\\right],\\;\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $H(\\cdot)$ is the Heaviside unit step function with\n",
    "\n",
    "$$\n",
    "H(x) =\n",
    "\\begin{cases}\n",
    "    1,& \\text{if } x>0\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_-dO56ledsy",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "\n",
    "## Part 2: Exam-Style Questions, Set 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CWV3624YZjhu",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Let $\\mathbf{p} = f(\\mathbf{x})$ be the output of the network's soft-max layer of some neural network classifier with $K$ layers when the network's input is $\\mathbf{x}$. The classifier's output is then\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max \\mathbf{p}\\;.\n",
    "$$\n",
    "\n",
    "If $y_n$ is the true label corresponding to training input $\\mathbf{x}_n$, the loss is $\\ell_n = \\ell(y_n, f(\\mathbf{x}_n))$ for some appropriate loss function $\\ell(y, \\mathbf{p})$.\n",
    "\n",
    "We saw in class that if $\\mathbf{x}^{(k)}$ is the output from layer $k$ and $\\mathbf{w}^{(k)}$ is a vector with all the parameters in layer $k$, then back-propagation computes the partial derivatives by the following recursion,  where $\\mathbf{x}^{(0)} = \\mathbf{x}$ is the input to the network and $\\mathbf{x}^{(K)} = \\mathbf{p}$:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial \\ell_n}{\\partial \\mathbf{w}^{(k)}} &=& \\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k)}} \\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{w}^{(k)}}\n",
    "\\;\\;\\;\\text{for}\\;\\;\\; k = K,\\ldots, 1 \\\\\n",
    "\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k-1)}} &=& \\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k)}} \\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{x}^{(k-1)}}\n",
    "\\;\\;\\;\\text{for}\\;\\;\\; k = K,\\ldots, 2\\\\\n",
    "\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(K)}} &=& \\frac{\\partial \\ell}{\\partial \\mathbf{p}}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The derivatives above are computed for the $n$-th training sample $(\\mathbf{x}_n, y_n)$ and for the values of $\\mathbf{w}^{(k)}$ that are current at any given point during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNK7pnfDesaH",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vY-vJZ_Peu9c",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Suppose that the network has only fully-connected layers (with ReLU nonlinearities) before the soft-max. Refer in detail to the equations given above to explain clearly why training would not work if the parameter vector $\\mathbf{w} = [\\mathbf{w}^{(1)},\\ldots, \\mathbf{w}^{(K)}]^T$ is initialized with zeros for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xEbEphZftUKn"
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpWSSew0tUKo"
   },
   "source": [
    "Because this network is connected via ReLU activated nonlinearities:\n",
    "\n",
    "$$\\mathbf{x}^{(k)} = ReLU(\\mathbf{w}^{(k)}_{1}*\\mathbf{x}^{(k-1)}+\\mathbf{w}^{(k)}_{2})\\;\\text{for}\\;\\; k = K-1,\\ldots, 2$$\n",
    "\n",
    "From this, we compute the value of $$\\mathbf{x}^{(k)} = ReLU(0*\\mathbf{x}^{(k-1)} + 0)=0\\;\\text{for}\\;\\; k = K-1,\\ldots, 2$$\n",
    "\n",
    "As the equation in the cell above demonstrates, it is necessary to have the term \n",
    "\n",
    "$\\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{x}^{(k-1)}}\n",
    "\\;\\;\\;\\text{defined for}\\;\\;\\; k = K,\\ldots, 2$\n",
    "\n",
    "When trying to compute such derivative:\n",
    "$$\\mathbf{x}^{(k)} = ReLU(\\mathbf{w}^{(k)}_{1}*\\mathbf{x}^{(k-1)}+\\mathbf{w}^{(k)}_{2})$$\n",
    "By the chain rule:\n",
    "$$\\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{x}^{(k-1)}} = \\frac{\\partial ReLU(\\mathbf{w}^{(k)}_{1}*\\mathbf{x}^{(k-1)}+\\mathbf{w}^{(k)}_{2})}{\\partial {x}^{(k-1)}}=\\mathbf{w}^{(k)}_{1}*\\frac{\\partial ReLU(\\mathbf{w}^{(k)}_{1}*\\mathbf{x}^{(k-1)}+\\mathbf{w}^{(k)}_{2})}{\\partial \\mathbf{w}^{(k)}_{1}*\\mathbf{x}^{(k-1)}+\\mathbf{w}^{(k)}_{2}} $$\n",
    "For simplicity, let us define \n",
    "$$\\mathbf{z}^{(k)}=\\mathbf{w}^{(k)}_{1}*\\mathbf{x}^{(k-1)}+\\mathbf{w}^{(k)}_{2}$$\n",
    "With this, we re-write the equation into:\n",
    "$$\\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{x}^{(k-1)}} =\\mathbf{w}^{(k)}_{1}*\\frac{\\partial ReLU(\\mathbf{z}^{(k)})}{\\partial \\mathbf{z}^{(k)}} $$\n",
    "\n",
    "At this point, we start to see problems with the training: $\\mathbf{z}^{(k)}=0$, and the derivative of the ReLU at 0 is not defined as the function is not differentiable at this point, meaning that the backpropagation calculations can not be computed at this point. \n",
    "A workaround at this point could be worked out however, by choosing the derivative to be either 1 or 0... Let's proceed to see if doing this would fix the issue.. In either case, \n",
    "$$\\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{x}^{(k-1)}} =\\mathbf{w}^{(k)}_{1}*\\frac{\\partial ReLU(\\mathbf{z}^{(k)})}{\\partial \\mathbf{z}^{(k)}}=0 $$\n",
    "since $$\\mathbf{w}^{(k)}_{1}=0$$\n",
    "We evaluate the expression:\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k-1)}} &=& \\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k)}} \\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{x}^{(k-1)}}\n",
    "\\;\\;\\;\\text{for}\\;\\;\\; k = K,\\ldots, 2\n",
    "\\end{eqnarray*}\n",
    "\n",
    "$$\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k-1)}} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7DPBwzDtUKp"
   },
   "source": [
    "We proceed with a similar analysis for the first equation from two cells above: \n",
    "$$\\frac{\\partial \\ell_n}{\\partial \\mathbf{w}^{(k)}} = \\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k)}} \\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{w}^{(k)}}\n",
    "\\;\\;\\;\\text{for}\\;\\;\\; k = K,\\ldots, 1 $$\n",
    "where, from the cell above we can conclude that:\n",
    "$$\\frac{\\partial \\ell_n}{\\partial \\mathbf{w}^{(k)}} = 0*\\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{w}^{(k)}}=0\n",
    "\\;\\;\\;\\text{for}\\;\\;\\; k = K-1,\\ldots, 1 $$\n",
    "The only case where there can be a non-zero gradient is for k = K\n",
    "\n",
    "We work to evaluate $$\\frac{\\partial \\mathbf{x}^{(K)}}{\\partial \\mathbf{w}^{(K)}}$$\n",
    "through the equation \n",
    "$$\\mathbf{x}^{(K)} = softMax(\\mathbf{w}^{(K)}_{1}*\\mathbf{x}^{(K-1)}+\\mathbf{w}^{(K)}_{2})$$\n",
    "we obtain\n",
    "$$\\frac{\\partial \\mathbf{x}^{(K)}}{\\partial \\mathbf{w}^{(K)}_1}=\\mathbf{x}^{(K-1)}*\\frac{\\partial softMax(\\mathbf{z}^{(K)})}{\\partial \\mathbf{z}^{(K)}}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{x}^{(K)}}{\\partial \\mathbf{w}^{(K)}_2}=\\frac{\\partial softMax(\\mathbf{z}^{(K)})}{\\partial \\mathbf{z}^{(K)}}$$\n",
    "After that, since\n",
    "$$\\mathbf{x}^{(K-1)} = ReLU(\\mathbf{w}^{(K-1)}_1*\\mathbf{x}^{(K-2)} + \\mathbf{w}^{(K-1)}_2)=0$$\n",
    "$$\\frac{\\partial \\mathbf{x}^{(K)}}{\\partial \\mathbf{w}^{(K)}_{1}}=0*\\frac{\\partial softMax(\\mathbf{z}^{(K)})}{\\partial \\mathbf{z}^{(K)}}=0$$\n",
    "and\n",
    "$$\\frac{\\partial \\ell_n}{\\partial \\mathbf{w}^{(K)}_{1}} =\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(K)}}*0=0$$\n",
    "\n",
    "This shows that the gain of the last layer has a gradient of 0 on the loss function, just like all of the weights in the previous layers.\n",
    "\n",
    "After the above analysis, the only possible parameter that can have a non-zero gradient is the bias in the very last layer: $\\mathbf{w}^{(K)}_{2}$. \n",
    "$$\\frac{\\partial \\ell_n}{\\partial \\mathbf{w}^{(K)}_{2}}=\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(K)}} \\frac{\\partial softMax(\\mathbf{z}^{(K)})}{\\partial \\mathbf{z}^{(K)}} = \\frac{\\partial \\ell}{\\partial \\mathbf{p}} \\frac{\\partial softMax(\\mathbf{z}^{(K)})}{\\partial \\mathbf{z}^{(K)}}$$\n",
    "This expression is not necessarily 0, as a matter of fact, it's almost certainly not 0 (depends on the definition of the loss function and/or whether the ground truth values are just zeros).\n",
    "\n",
    "Since we have a parameter with a non-zero gradient, the algorithm can optimize it. But even after the algorithm tweaks this parameter, the analysis in this cell and the previous one will still hold on the rest of parameters, meaning this bias (or biases, depending on the number of neurons in the last layer) will be the only parameter that will be changed in the optimization--rendering the neural network useless for all other weights.\n",
    "\n",
    "Reminder: The algorithm won't change parameters with a loss gradient of zero--it treats them as local minima. Remember that the magnitude of variation in a parameter from iteration to iteration is set by the derivative of the loss function on it--which is why none of our weights will change (other than $\\mathbf{w}^{(K)}_{2}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Vx-msEHjfdn",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJnaTzLfjiGt",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "A neural net classifier with only fully-connected layers (with ReLU nonlinearities) and a soft-max layer at its output has parameter vector $\\mathbf{w}$, and the network implements the function $f(\\mathbf{x}, \\mathbf{w})$ for any network input $\\mathbf{x}$. Is $\\mathbf{w} = \\mathbf{0}$ a stationary point for the function $\\phi(\\mathbf{w}) = f(\\mathbf{x}, \\mathbf{w})$ when $\\mathbf{x}$ is fixed? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxjAkNkzkDHo",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yvy4eEhWtUKx"
   },
   "source": [
    "This answer depends on whether the neural network has bias weights or not.\n",
    "\n",
    "- If the network does not include biases:\n",
    "    Yes, this is a stationary point because the gradient of the loss function from every single weight in the system is 0 (see answer from previous problem).\n",
    "    \n",
    "    \n",
    "- If the network includes biases in its weights:\n",
    "    No, as shown in the previous problem's answer, the bias from the very last layer will have a non-zero gradient and therefore this will not be a stationary point and it will optimize for that weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rla5OsKGk1zi",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lE3Uoh6Hk5jp",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Stochastic gradient descent with momentum is used to train a certain neural network with $m$ parameters. Just before iteration $t$ of training is performed, the parameter vector has value $\\mathbf{w}_t$, and the velocity (or step) is $\\mathbf{v}_t = \\mathbf{a}$, where $\\mathbf{a}$ is some nonzero vector in $\\mathbb{R}^m$ (refer to the class notes for notation). The momentum coefficient is kept constant at $\\mu = 0.9$ throughout training. If the risk function has a saddle point at $\\mathbf{w}_t$, what is the step $\\mathbf{v}_{t+1}$ at iteration $t$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTU6DAPRn1hN",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9iMc8x6RtUK7"
   },
   "source": [
    "Since the risk function is a saddle point, $\\nabla L_{T}(\\mathbf{w}_{t})=0$ \n",
    "$$\\mathbf{v}_{t+1} = \\mu_{t}\\mathbf{v}_{t}-\\alpha \\nabla L_{T}(\\mathbf{w}_{t})=0.9\\mathbf{a}-\\alpha*0=0.9\\mathbf{a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwMq7SK4nFwU",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iEaE7twinHyl",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "A friend of yours argues that in the situation described in the previous problem, the steps after iteration $t$ decay exponentially. Her argument is based on the fact that the risk is at a saddle point at $\\mathbf{w}_t$, and the momentum coefficient is constant, so that $\\mathbf{v}_{t+\\tau} = \\mu^{\\tau}\\mathbf{a}$, an exponential decay. Explain why your friend's argument is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uyybk8rYtUK_"
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qp_v4UdgtULB"
   },
   "source": [
    "Her argument is wrong because after iteration t, we will have overshot the saddle point and at the next iteration we will no longer be in a saddle point, which is the basis of her argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfRyRIt3qCoz",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWfz9zLXqFAV",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "In the situation described in Problem 2.3, will the training algorithm always eventually converge back towards $\\mathbf{w}_t$? Explain your answer briefly and clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zv-spO4mqZcT",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_6ApD31tULH"
   },
   "source": [
    "No. If the momentum variable is very large, the energy of the step in iteration t might be so strong that it moves away from the valley that the local saddle is located--moving to a different local minimum.\n",
    "Additionally, the saddle is not necessarily a local minimum, in which case overshooting it could simply mean that the algorithm entered a gradient that is already stepping away from that point $\\mathbf{w}_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AMarkOTyVt3",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "## Part 3: Exam-Style Questions, Set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74EwMXGkyYgz",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "The following problems take you through the computation of the set of all least-squares solutions to the following linear system:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "3x + 4y &=& 2\\\\\n",
    "3x + 4y &=& 3\n",
    "\\end{eqnarray*}\n",
    "\n",
    "and the solutions to a related optimization problem.\n",
    "All the answers to the questions in this problem are numerical and exact. They refer only to the data given in the problem, and no more general answers are required. You may leave your answers in the form of fractions, with expressions like the following:\n",
    "\n",
    "$$\n",
    "\\frac{\\sqrt{3}}{2} \\left[\\begin{array}{c} 2\\\\-5\\end{array}\\right]\\;,\n",
    "$$\n",
    "\n",
    "but please simplify as much as possible.\n",
    "\n",
    "_As usual, it is easiest to answer these questions using software (and perhaps guess the exact values from the approximate ones output by your code). However, this would rob you of the opportunity to understand this material and to practice for the exam. **In any event, no answers will be accepted to problems in this part that embed software in your submission.**_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ac8j8SGdzDr9",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9R-zasYizH16",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "What are $A$ and $\\mathbf{b}$ if we write the system in this problem in the following form?\n",
    "$$\n",
    "A \\mathbf{x} = \\mathbf{b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6jJC2xuzKS5",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_s5FxILHtULT"
   },
   "source": [
    "$$\n",
    "A = \\left[\\begin{array}\n",
    "& 3 & 4 \\\\\n",
    "3 & 4\n",
    "\\end{array}\\right],\\\\\n",
    "b = \\left[\\begin{array}\n",
    "& 2 \\\\\n",
    "3\n",
    "\\end{array}\\right].\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJoEvX1bzPDh",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zA4CavTxzReO",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "What is the rank of $A$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aplEQX65zUHW",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJWPuK-1tULc"
   },
   "source": [
    "$$\n",
    "A = \\left[\\begin{array}\n",
    "& 3 & 4 \\\\\n",
    "3 & 4\n",
    "\\end{array}\\right] \\Rightarrow \\left[\\begin{array}\n",
    "& 3 & 4 \\\\\n",
    "0 & 0\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\therefore \\; rank(A) = 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOrmLSUVzWM6",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUH5dslBzapt",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Give a _unit_ column vector $\\mathbf{r}$ that spans the row space of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7vpRTgKzfEi",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eXO13jPtULi"
   },
   "source": [
    "Since \n",
    "\n",
    "$$\n",
    "Row(A) = Col(A^T) = Col\\left\\{{\\left[\\begin{array}\n",
    "& 3 & 3 \\\\\n",
    "4 & 4\n",
    "\\end{array}\\right]}\\right\\},\\\\\n",
    "$$\n",
    "\n",
    "we have\n",
    "\n",
    "$$\n",
    "Row(A) = Span \\left\\{ \\mathbf{r} \\right\\},\\;\\;\\; \\mathbf{r} = \\left[\\begin{array}{c} 3/5 \\\\ 4/5\n",
    "\\end{array}\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbW0YgUWzjQc",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6kAPv09zlWk",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Give a _unit_ column vector $\\mathbf{n}$ that spans the null space of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23yZJ6GCzo2K",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrhX0Dy4tUL0"
   },
   "source": [
    "From\n",
    "\n",
    "$$\n",
    "A\\mathbf{x} = \\mathbf{0},\n",
    "$$\n",
    "\n",
    "we get\n",
    "\n",
    "$$\n",
    "3x_1 + 4x_2 = 0\\Leftrightarrow x_1 = -\\frac{4}{3}x_2, \\\\\n",
    "\\mathbf{x} = x_2\\left[\\begin{array}{c}\n",
    "-4/3 \\\\\n",
    "1\n",
    "\\end{array}\\right].\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "Nul(A) = Span \\left\\{ \\mathbf{n} \\right\\},\\;\\;\\; \\mathbf{n} = \\left[\\begin{array}{c} -4/5 \\\\ 3/5\n",
    "\\end{array}\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nOm2a0USzwU0",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMNnKIARzyfL",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Write the matrix $V$ in the SVD $A = U\\Sigma V^T$ of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c7U8WqRUz0ct",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NaPeCXMtUL_"
   },
   "source": [
    "Taking SVD of $A\\in\\mathbb{R}^{2\\times2}$ yields:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A &= U\\Sigma V^T \\\\\n",
    "&= \n",
    "[\\mathbf{u}_1\\;\\mathbf{u}_2]\n",
    "\\left[\\begin{array}{cc} \\sigma_1 & 0 \\\\ 0 & \\sigma_2=0 \\end{array}\\right]\n",
    "\\left[\\begin{array}{c} \\mathbf{v}_1^T \\\\ \\mathbf{v}_2^T \\end{array}\\right].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{v}_1$ spans $Row(A)$ and $\\mathbf{v}_2$ spans $Nul(A)$,\n",
    "\n",
    "$$\n",
    "V = \\left[\\begin{array}{cc} 3/5 & -4/5 \\\\ 4/5 & 3/5 \\end{array}\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zk41d7sBz5GS",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZK3cYMIkz7Jc",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Compute the matrices $U$ and $\\Sigma$ in the SVD of $A$. [Hint: compute $U\\Sigma$ first.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T3ZNlDaUz9gQ",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x3LAPeVptUMa"
   },
   "source": [
    "Since $\\mathbf{u}_1$ spans $Col(A)$, $\\mathbf{u}_1 = \\left[\\begin{array}{c} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{array}\\right]$.\n",
    "\n",
    "Since $\\mathbf{u}_2$ spans $LeftNul(A)$ ($\\Leftrightarrow Nul(A^T)$), $\\mathbf{u}_2 = \\left[\\begin{array}{c} -1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{array}\\right]$.\n",
    "\n",
    "Therefore, we get\n",
    "\n",
    "$$\n",
    "U = \\left[\\begin{array}{cc} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{array}\\right].\n",
    "$$\n",
    "\n",
    "Compute $U\\Sigma$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U\\Sigma &= AV = \\left[\\begin{array}{cc} 5 & 0 \\\\ 5 & 0 \\end{array}\\right] \\\\\n",
    "&= \\left[\\begin{array}{cc} \\sigma_1/\\sqrt{2} & 0 \\\\ \\sigma_1/\\sqrt{2} & 0 \\end{array}\\right].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\Sigma = \\left[\\begin{array}{cc} 5\\sqrt{2} & 0 \\\\ 0 & 0 \\end{array}\\right].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHCt-FvH0FDT",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EhxCKF6U0HwH",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Compute the pseudo-inverse $A^{\\dagger}$ of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "63rXWdST0KZN",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUNgwA5GtUMg"
   },
   "source": [
    "The pseudo-inverse of $A$ can be computed as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A^{\\dagger} &= V\\Sigma^{\\dagger}U^T \\\\\n",
    "&= \\left[\\begin{array}{cc} 3/5 & -4/5 \\\\ 4/5 & 3/5 \\end{array}\\right]\n",
    "\\left[\\begin{array}{cc} 5\\sqrt{2} & 0 \\\\ 0 & 0 \\end{array}\\right]\n",
    "\\left[\\begin{array}{cc} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{array}\\right] \\\\\n",
    "&=\\left[\\begin{array}{cc} 3/50 & 3/50 \\\\ 4/50 & 4/50 \\end{array}\\right].\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hllY12AA0PKi",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MnjLAWvt0Q06",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Find the minimum-norm least-squares solution $\\mathbf{x}^*$ of the system $A\\mathbf{x} = \\mathbf{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2RgECzBj0TVw",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5-1w_eztUMl"
   },
   "source": [
    "The minimum-norm least-squares solution $\\mathbf{x}^*$ can be computed as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x}^* &= A^{\\dagger}\\mathbf{b} \\\\\n",
    "&= \\left[\\begin{array}{cc} 3/50 & 3/50 \\\\ 4/50 & 4/50 \\end{array}\\right]\n",
    "\\left[\\begin{array}{c} 2 \\\\ 3 \\end{array}\\right]\\\\\n",
    "&= \\left[\\begin{array}{c} 3/10 \\\\ 4/10 \\end{array}\\right]\\\\\n",
    "\\end{align}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDz6wROp0ZIr",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0P5mlHPz0bTA",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Give an expression for the set $S$ of all least-squares solutions of the system $A\\mathbf{x} = \\mathbf{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K2ZKErfi0dwK",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-m6JXOgtUM0"
   },
   "source": [
    "We can re-write the least-squares problem as:\n",
    "\n",
    "$$\n",
    "\\min \\|A\\mathbf{x}-\\mathbf{b}\\| = \\min\\|\\Sigma \\mathbf{y}-\\mathbf{c}\\|, \\; \\text{where}\\; \\mathbf{y}=V^T\\mathbf{x}, \\mathbf{c}=U^T\\mathbf{b}.\n",
    "$$\n",
    "\n",
    "Let's look closely at $\\Sigma \\mathbf{y}-\\mathbf{c}$:\n",
    "\n",
    "$$\n",
    "\\Sigma \\mathbf{y}-\\mathbf{c} = \\left[\\begin{array}{cc} \\sigma_1 & 0 \\\\ 0 & 0 \\end{array}\\right]\n",
    "\\left[\\begin{array}{c} y_1 \\\\ y_2 \\end{array}\\right] - \n",
    "\\left[\\begin{array}{c} c_1 \\\\ c_2 \\end{array}\\right].\n",
    "$$\n",
    "\n",
    "Although $y_1$ should be chosen as $c_1/\\sigma_1$ to minimize the overall norm, since $y_2$ does not affect the norm at all, it can be chosen however.\n",
    "\n",
    "Let's compute $\\mathbf{c}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left[\\begin{array}{c} c_1 \\\\ c_2 \\end{array}\\right] &= \n",
    "\\left[\\begin{array}{cc} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{array}\\right]\n",
    "\\left[\\begin{array}{c} 2 \\\\ 3 \\end{array}\\right] \\\\\n",
    "&= \\left[\\begin{array}{c} 5/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{array}\\right].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Find expression for $\\mathbf{y}$:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{c} y_1 \\\\ y_2 \\end{array}\\right] =\n",
    "\\left[\\begin{array}{cc} 3/5 & 4/5 \\\\ -4/5 & 3/5 \\end{array}\\right]\n",
    "\\left[\\begin{array}{c} x_1 \\\\ x_2 \\end{array}\\right] \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_1 &= \\frac{3}{5}x_1 + \\frac{4}{5}x_2 \\\\\n",
    "&= \\frac{c_1}{\\sigma_1} = \\frac{1}{2} \\\\\n",
    "y_2 &= -\\frac{4}{5}x_1 + \\frac{3}{5}x_2 \\\\\n",
    "&= k,\\;\\;\\forall k\\in\\mathbb{R}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "From above, we can find an expression for $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 &= \\frac{3}{10} - \\frac{4}{5}k \\\\\n",
    "x_2 &= \\frac{2}{5} + \\frac{3}{5}k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\therefore \\hat{\\mathbf{x}} = \\left[\\begin{array}{c} 3/10 \\\\ 4/10 \\end{array}\\right] + k \\left[\\begin{array}{c} -4/5 \\\\ 3/5 \\end{array}\\right], \\;\\;\\forall k\\in\\mathbb{R}.\n",
    "$$\n",
    "\n",
    "Note that when $k=0$, we get the minimum-norm least-squares solution $\\mathbf{x}^* =\\left[\\begin{array}{c} 3/10 \\\\ 4/10 \\end{array}\\right]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8vQg6So00ij9",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghlx7z5s0lkG",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Find all the solutions to\n",
    "\n",
    "$$\n",
    " \\hat{\\mathbf{x}} = \\arg\\min_{\\|\\mathbf{x}\\| = 1} \\|A\\mathbf{x}\\|\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXELtk700oFd",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b0xd0TeWtUNF"
   },
   "source": [
    "The unit-norm least-squares solution to the homogeneous linear system system $A\\mathbf{x}=0$ corresponds to the last column $\\mathbf{v}_2$ of right singular vector matrix $V$.\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = \\left[\\begin{array}{c} -4/5 \\\\ 3/5 \\end{array}\\right],\\; \n",
    "\\left[\\begin{array}{c} 4/5 \\\\ -3/5 \\end{array}\\right],\\;\n",
    "\\left[\\begin{array}{c} 4/5 \\\\ 3/5 \\end{array}\\right],\\;\\text{and}\\; \n",
    "\\left[\\begin{array}{c} -4/5 \\\\ -3/5 \\end{array}\\right].\n",
    "$$\n",
    "\n",
    "Multiple solutions are possible due to sign ambiguity of SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GZdQ7IsoUVR",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "## Part 4: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4gQIvWQAzhzm",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "The code in this part is somewhat modified from the [Keras documentation](https://keras.io/examples/cifar10_cnn/). It downloads the CIFAR-10 dataset, a set of 60000 labeled images grouped in 10 categories, which it splits into training, validation, and test sets. It then defines a function `network` that returns a simple convolutional neural network (the `model`), and a function `train` that trains the model for a single epoch by default, checking performance on the validation set.The function `train` also saves the trained model in a file in the cloud and evaluates the model on the test data. Finally, it returns a history of training and validation accuracies achieved after each epoch of training. The function `train` uses SGD as the default optimizer.\n",
    "\n",
    "_**Important:**_ Make sure you select Python 3 through the `Runtime->Change runtime type` menu at the top of the notebook. Also set the hardware acceleration to `None` in that same menu. We will turn on GPU acceleration later on. TPU acceleration is not always available, so we won't use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "JM7Sh_kizkO-",
    "outputId": "43392317-a9d7-4ef8-c7f2-b84e987221e8",
    "tags": [
     "HST"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 96s 1us/step\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "40000 training samples\n",
      "10000 validation samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# The data, split between train, validation, and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "(x_train, x_validate, y_train, y_validate) = train_test_split(x_train, y_train,\n",
    "                                                             test_size=0.2)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'training samples')\n",
    "print(x_validate.shape[0], 'validation samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_validate = keras.utils.to_categorical(y_validate, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_validate = x_validate.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_validate /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "FBMRAFNN5k6S",
    "outputId": "5f531cbb-7cdd-4e15-cb51-64c6905e7454",
    "tags": [
     "HST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "activation_function = 'relu'\n",
    "\n",
    "def network(activation_function='relu'):\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                  input_shape=x_train.shape[1:]))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(Conv2D(32, (3, 3)))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(Conv2D(64, (3, 3)))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(num_classes))\n",
    "  model.add(Activation('softmax'))\n",
    "  \n",
    "  return model\n",
    "\n",
    "model = network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35sd9xJr55eM",
    "tags": [
     "HST"
    ]
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "def train(model, epochs=1,\n",
    "          opt = keras.optimizers.SGD(lr=0.01, momentum=0.7, decay=0.001),\n",
    "          verbose=2):\n",
    "\n",
    "  batch_size = 32\n",
    "  save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "  model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "  # Configure the model for training\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  history = model.fit(x_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(x_validate, y_validate),\n",
    "                      shuffle=False,\n",
    "                      verbose=verbose)\n",
    "\n",
    "  # Save model and weights\n",
    "  if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "  model_path = os.path.join(save_dir, model_name)\n",
    "  model.save(model_path)\n",
    "\n",
    "  # Score trained model.\n",
    "  scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "  print('Test loss:', scores[0])\n",
    "  print('Test accuracy:', scores[1])\n",
    "  return [history.epoch, history.history['acc'], history.history['val_acc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JMElXg2ETjAE",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oDnPx1gdT5Jm",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Using Stochastic Gradient Descent (SGD) with the default parameters in `train`, train the model for one epoch _with no hardware acceleration_.\n",
    "\n",
    "Show your call to `train` and the outputs it generates. Is validation accuracy a reasonably good estimate of test accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73TQApgKiGY_",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "#### Programming Notes\n",
    "\n",
    "+ Hardware acceleration is turned off through the `Runtime->Change runtime type` menu at the top of the notebook, and selecting `None` for hardware acceleration.\n",
    "\n",
    "+ Depending on circumstances, after you change the runtime type, some or all of the notebook state may be lost. This will require you to rerun some of the cells above.\n",
    "\n",
    "+ Tensorflow may generate warning messages that depend on how the Colaboratory interface is implemented. These messages are typically harmless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5sYqNFyUVxdy",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "deIo6eEp-6cs",
    "outputId": "4778c893-906f-419d-b9a0-9c83b134031c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 213s - loss: 1.9953 - acc: 0.2680 - val_loss: 1.6989 - val_acc: 0.3925\n",
      "10000/10000 [==============================] - 15s 2ms/step\n",
      "Test loss: 1.6941104961395264\n",
      "Test accuracy: 0.3906\n"
     ]
    }
   ],
   "source": [
    "model = network()\n",
    "[epoc_out, acc, val_acc] = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iYaIRxEeMNK7",
    "outputId": "a6b7305b-12dd-42c7-9051-26e9397bb9e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cUFabarBN9By",
    "outputId": "f0aaf436-8f1d-42c2-a13f-f3a473abd7f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.268025]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tvwjvV_COHUD",
    "outputId": "5b3d8b69-a16e-4d48-ea09-1270d1737e0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3925]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfydNOiwO5gN"
   },
   "source": [
    "Yes. Validation accuracy is a reasonably good estimate of test accuracy in this case. You can see how the 0.3983 for test accuracy is close to the 0.3894 accuracy for validation. This assumption that validation accuracy isa . good estimate of test accuracy usually holds true--asusming that they both come from similar data. \n",
    "However, after a lot of messing with the nueral network's architecture in hopes of increasing validation accuracy, this might be less true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NoGPF3W7WU_X",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yCDoSETDWXCJ",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Repeat the previous experiment _after turning on GPU acceleration_ from the `Runtime->Change runtime type` menu.\n",
    "\n",
    "Are the accuracy values the same as before? Explain why or why not. What is the approximate ratio of running times of CPU (no acceleration) versus GPU training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRnVqg5gXbvx",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Bjk8OzlKZMNH",
    "outputId": "582e1ee5-cdc6-491b-b726-b4b7b2e10c2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 44s 0us/step\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "40000 training samples\n",
      "10000 validation samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# The data, split between train, validation, and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "(x_train, x_validate, y_train, y_validate) = train_test_split(x_train, y_train,\n",
    "                                                             test_size=0.2)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'training samples')\n",
    "print(x_validate.shape[0], 'validation samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_validate = keras.utils.to_categorical(y_validate, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_validate = x_validate.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_validate /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "sq_K-pqMZO3W",
    "outputId": "9a480480-0b5e-491c-99e5-cdfb87d162b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "activation_function = 'relu'\n",
    "\n",
    "def network(activation_function='relu'):\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                  input_shape=x_train.shape[1:]))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(Conv2D(32, (3, 3)))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(Conv2D(64, (3, 3)))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(num_classes))\n",
    "  model.add(Activation('softmax'))\n",
    "  \n",
    "  return model\n",
    "\n",
    "model = network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W1huPGTIZRwr"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "def train(model, epochs=1,\n",
    "          opt = keras.optimizers.SGD(lr=0.01, momentum=0.7, decay=0.001),\n",
    "          verbose=2):\n",
    "\n",
    "  batch_size = 32\n",
    "  save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "  model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "  # Configure the model for training\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  history = model.fit(x_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(x_validate, y_validate),\n",
    "                      shuffle=False,\n",
    "                      verbose=verbose)\n",
    "\n",
    "  # Save model and weights\n",
    "  if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "  model_path = os.path.join(save_dir, model_name)\n",
    "  model.save(model_path)\n",
    "\n",
    "  # Score trained model.\n",
    "  scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "  print('Test loss:', scores[0])\n",
    "  print('Test accuracy:', scores[1])\n",
    "  return [history.epoch, history.history['acc'], history.history['val_acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "lb59jfFtUAQS",
    "outputId": "a41ec16d-ea45-4971-9e44-09218d496fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 17s - loss: 1.9964 - acc: 0.2652 - val_loss: 1.6874 - val_acc: 0.3915\n",
      "10000/10000 [==============================] - 1s 119us/step\n",
      "Test loss: 1.6719089736938477\n",
      "Test accuracy: 0.3972\n"
     ]
    }
   ],
   "source": [
    "# model = network() ## model has already been initialized, so no need to do it again.. This line is here so the grader can see that we didn't forget to initialize\n",
    "[epoc_out, acc, val_acc] = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BLPDkYDvWohZ",
    "outputId": "48389761-49f5-4eb9-f141-9316a2128e89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.265175]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8uKGng0NWmz_",
    "outputId": "e83f7fc4-a782-42ec-f6de-468f44303680"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3915]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7cn4kM_dUugF",
    "outputId": "d3dbb09e-d834-4635-d8ad-276258ff6573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approximate ratio of running times of GPU to CPU is: 8.889415669076685\n"
     ]
    }
   ],
   "source": [
    "gpu_cpu_ratio = 15 / 1.6874\n",
    "print('The approximate ratio of running times of GPU to CPU is: ' + str(gpu_cpu_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tN6bb7EEaksW"
   },
   "source": [
    "The accuracies are different in each case. The reason for this is that the initialization of the parameters in the network is randomized each time. This means the optimization for each run started at a different configuration and therefore shouldn't be expected to end up at the exact location after one epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UL293H0_YerN",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hw67mTPzYg-4",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "We keep GPU acceleration turned on from now on.\n",
    "\n",
    "Repeat the experiment above with the ADAM optimizer with the default parameters. This optimizer selects the descent step size adaptively. The ADAM optimizer is invoked by using parameter `opt = keras.optimizers.Adam()` in `train`.\n",
    "\n",
    "Compare accuracies and running times with those achieved in the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRFT6uhiZdIj",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "i50p6thsMIet",
    "outputId": "e057c3b3-e0c4-4cb2-ee81-be5ba23d8601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 15s - loss: 1.5826 - acc: 0.4234 - val_loss: 1.1923 - val_acc: 0.5689\n",
      "10000/10000 [==============================] - 1s 124us/step\n",
      "Test loss: 1.1892164852142333\n",
      "Test accuracy: 0.5711\n"
     ]
    }
   ],
   "source": [
    "model = network()\n",
    "[epoc_out, acc, val_acc] = train(model, opt = keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BSX6xb8Uv1vz"
   },
   "source": [
    "The accuracy is different as it was in our experiment with the SGD. In the ADAM case, the accuracy is larger (0.5711>0.3972), which suggests it's a better optimizer for this particular network.\n",
    "\n",
    "The running time is not really very different (15s vs 17s, and 1s per epoch vs 1s per epoch) in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KEp7_h36bC7R",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxdkfA9ibFUM",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "We use the ADAM optimizer with default parameters from now on.\n",
    "\n",
    "Repeat the previous experiment with 30 epochs of training instead of 1 (`epochs=30`). This time, store the value returned by `train`, as you will need it for plotting.\n",
    "\n",
    "When done, plot both training accuracy and validation accuracy as functions of epoch number on the same diagram. Label the axes and add a legend to specify which plot is which.\n",
    "\n",
    "Do you think that the classifier would perform much better if you were to train longer? Explain briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pw9glyaNvwQ6",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "#### Programming Notes\n",
    "\n",
    "+ Look at the definition of `train` to figure out what the output from that function contains.\n",
    "+ Set the value of the `verbose` parameter in the call to `train` to 0 to suppress output, which would be too long to include in your PDF file. You can estimate from your previous experiments how long the code will take to run. Alternatively, set `verbose` to 2 in early test runs, but then set it to 0 in your final run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pxOicSFohW3T",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3G0Mh-2hQW0V",
    "outputId": "e2aef2dd-4bc9-445d-b942-ea44078d3a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 125us/step\n",
      "Test loss: 0.7619545970916748\n",
      "Test accuracy: 0.7689\n"
     ]
    }
   ],
   "source": [
    "model = network()\n",
    "[epoc_out, acc, val_acc] = train(model, opt = keras.optimizers.Adam(), epochs = 30, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "mfqb3sEsPwrV",
    "outputId": "d3e3928f-715f-41d9-99f0-8eb35cbe6ad5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plots for training and testing process: loss and accuracy\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(0)\n",
    "plt.plot(acc,'r')\n",
    "plt.plot(val_acc,'g')\n",
    "plt.xticks(np.arange(0, 31, 2.0))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy vs Validation Accuracy\")\n",
    "plt.legend(['train','validation'])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dV87zWIbXBIV"
   },
   "source": [
    "I do not think this would be much better if you trained it longer. As evidenced in the graph, the accuracy measures begin to plateau--especially the validation accuracy. This is suggestive of over-fitting, which means training more will not help. (But even the train accuracy plateaus.. so even if there was no overfitting, it still wouldn't make much sense to keep training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxdW8dV4uoCq",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A97WOgV8uweq",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Suggest at least three different ways to improve the performance of the classifier defined in this Part. For each way, explain why that would help. This is an open-ended question, and answers may vary. Do _not_ implement your suggestions, and do _not_ refer to techniques we have not covered in class (such as batch normalization or other techniques you may have heard of).\n",
    "\n",
    "If you suggest more than three ways, we will grade you for the best ones. However, we _will_ deduct points for patently wrong statements in any of your suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12XxnmJwnYzN",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZWLsXuXRleX"
   },
   "source": [
    "1. Adding more convolutional layers in the architecture: the more layers there are, the more parameters the system can optimize for, which will make the model more flexible. The only drawbacks might be training time and overfitting--however, since our dataset is so large, we are less exposed to overfitting.\n",
    "\n",
    "2. Change the allocation of validation and training samples. By increasing the number of training images you might be able to improve the accuracy. The drawback will be that you have a less trust-worthy validation to determine how generalizable the results are.\n",
    "\n",
    "3. Increase the number of neurons in each convolutional layer. You can do that by increasing the number of filters in each layer. This will make the model more powerful and flexible to increase the accuracy."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "homework05.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
