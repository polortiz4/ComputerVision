{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework05.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/polortiz4/ComputerVision/blob/master/homework05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_4pJohDkIbwu",
        "tags": [
          "T"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "> **Student Names and IDs**:\n",
        ">\n",
        "> - Pablo Ortiz, 0686443\n",
        "> - Jongwan Park, 0848815"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "93F4Xn6mIbww",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "# Homework 5"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "aymIv7MFIbwz",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "## Homework Submission Workflow\n",
        "\n",
        "When you submit your work, follow the instructions on the [submission workflow page](https://www.cs.duke.edu/courses/fall18/compsci371d/homework/workflow.html) for full credit, but see the changes mentioned below.\n",
        "\n",
        "**Important: Failure to do any of the following will result in lost points:**\n",
        "\n",
        "- Submit **one** PDF file and **one** notebook per group\n",
        "\n",
        "- Enter **all the group members** through the Gradescope GUI when you submit your PDF files. It is **not** enough to list group members in your documents\n",
        "\n",
        "- Match each **answer** (not question!) with the appropriate page in Gradescope\n",
        "\n",
        "- Avoid large blank spaces in your PDF file\n",
        "\n",
        "**Important changes to homework preparation workflow:** _This assignment is different from the others in that you are required to run it on the Google Colaboratory, a cloud service that Google makes available for reseach in machine learning. This is necessary because some of the problems require you to train a deep network on hardware that is faster than what is typically available on a standard laptop or desktop, including a high-end GPU. Even if you do have a high-end GPU, please run your notebook on the Colaboratory, so we can grade your work consistently._\n",
        "\n",
        "_**To work on this assignment, go to the [Colaboratory](https://colab.research.google.com) and upload the template notebook for this assignment through the `File` menu at the top of the Colaboratory page. Then work on the assignment, making sure to pay attention to instructions in Part 4 where you are asked to change the runtime type.**_\n",
        "\n",
        "_**When you are done, download the notebook (after making sure that all the outputs from running the code show up properly), and proceed as usual to turn that notebook into a PDF file for submission.**_\n",
        "\n",
        "#### Programming Notes\n",
        "\n",
        "+ The Colaboratory is a cloud service. If a notebook sits idle for a long time, it automatically disconnects from its execution kernel, and you need to rerun all the cells.\n",
        "+ Some of the cells in the Part on neural networks are to be run with different runtime types, as explained later. Because of this, you will not be able to just restart the notebook and run all its cells with a single command. Instead, you need to run the cells one at a time, changing runtime type as instructed. Make sure you do this once you are done with the assignment, making sure that the output from your code matches the text where you describe that output.\n",
        "+ Depending on circumstances, changing the runtime type may erase some or all of the notebook state. This will require you to rerun the cells that generate state.\n",
        "+ Training depends on random initialization of the network parameters. Because of this, your results may vary relative to the sample solution, even if your code is no different. Results may also vary from run to run."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gIp6dzlwIbw0",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1: Exam-Style Questions, Set 1"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6o90sGDiK4aF",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "The small neural net in the figure below uses the ReLU as the nonlinearity at the output of each neuron. The values specified in the hollow circles are biases, and the values along the edges are gains. Weigths number 1, 2, 3 refer to the first neuron, 4, 5,6 to the second, 7, 8, 9 to the third."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CakDYmy0Mk13",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "![a simple neural network](https://www2.cs.duke.edu/courses/spring19/compsci527/homework/5/netSimple.png)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tdrtWLQ5JZ7q",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 1.1"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CacoODWaJdmL",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Are all the layers in the net above fully connected?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WJ9w1-GpN1FM",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4o_0Co16OJ-v",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 1.2"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WmVrDJydN7OP",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "What is the output $y$ from the net above when the input is as follows?\n",
        "\n",
        "$$\n",
        "x_1 = 0 \\;\\;\\; \\text{and}\\;\\;\\; x_2 = 3\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "G-LZGjFMORa8",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rP4qW1wUQapl",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 1.3"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2WZXG8wNQeJ5",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "What is the gradient $\\mathbf{g}$ of the output $y$ of the network above with respect to the weight vector\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = [w_1,\\ w_2,\\ w_3,\\ w_4,\\ w_5,\\ w_6,\\ w_7,\\ w_8,\\ w_9]^T\n",
        "$$\n",
        "\n",
        "when the input has the values given in the previous problem? Just give the result if you are confident of your answer."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5fVXbN93RDq7",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "j_-dO56ledsy",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Part 2: Exam-Style Questions, Set 2\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CWV3624YZjhu",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Let $\\mathbf{p} = f(\\mathbf{x})$ be the output of the network's soft-max layer of some neural network classifier with $K$ layers when the network's input is $\\mathbf{x}$. The classifier's output is then\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\arg\\max \\mathbf{p}\\;.\n",
        "$$\n",
        "\n",
        "If $y_n$ is the true label corresponding to training input $\\mathbf{x}_n$, the loss is $\\ell_n = \\ell(y_n, f(\\mathbf{x}_n))$ for some appropriate loss function $\\ell(y, \\mathbf{p})$.\n",
        "\n",
        "We saw in class that if $\\mathbf{x}^{(k)}$ is the output from layer $k$ and $\\mathbf{w}^{(k)}$ is a vector with all the parameters in layer $k$, then back-propagation computes the partial derivatives by the following recursion,  where $\\mathbf{x}^{(0)} = \\mathbf{x}$ is the input to the network and $\\mathbf{x}^{(K)} = \\mathbf{p}$:\n",
        "\n",
        "\\begin{eqnarray*}\n",
        "\\frac{\\partial \\ell_n}{\\partial \\mathbf{w}^{(k)}} &=& \\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k)}} \\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{w}^{(k)}}\n",
        "\\;\\;\\;\\text{for}\\;\\;\\; k = K,\\ldots, 1 \\\\\n",
        "\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k-1)}} &=& \\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k)}} \\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{x}^{(k-1)}}\n",
        "\\;\\;\\;\\text{for}\\;\\;\\; k = K,\\ldots, 2\\\\\n",
        "\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(K)}} &=& \\frac{\\partial \\ell}{\\partial \\mathbf{p}}\n",
        "\\end{eqnarray*}\n",
        "\n",
        "The derivatives above are computed for the $n$-th training sample $(\\mathbf{x}_n, y_n)$ and for the values of $\\mathbf{w}^{(k)}$ that are current at any given point during training."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cNK7pnfDesaH",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 2.1"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vY-vJZ_Peu9c",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Suppose that the network has only fully-connected layers (with ReLU nonlinearities) before the soft-max. Refer in detail to the equations given above to explain clearly why training would not work if the parameter vector $\\mathbf{w} = [\\mathbf{w}^{(1)},\\ldots, \\mathbf{w}^{(K)}]^T$ is initialized with zeros for training."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "I5CVu5NXhUpK",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2Vx-msEHjfdn",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 2.2"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NJnaTzLfjiGt",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "A neural net classifier with only fully-connected layers (with ReLU nonlinearities) and a soft-max layer at its output has parameter vector $\\mathbf{w}$, and the network implements the function $f(\\mathbf{x}, \\mathbf{w})$ for any network input $\\mathbf{x}$. Is $\\mathbf{w} = \\mathbf{0}$ a stationary point for the function $\\phi(\\mathbf{w}) = f(\\mathbf{x}, \\mathbf{w})$ when $\\mathbf{x}$ is fixed? Justify your answer."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OxjAkNkzkDHo",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Rla5OsKGk1zi",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 2.3"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lE3Uoh6Hk5jp",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Stochastic gradient descent with momentum is used to train a certain neural network with $m$ parameters. Just before iteration $t$ of training is performed, the parameter vector has value $\\mathbf{w}_t$, and the velocity (or step) is $\\mathbf{v}_t = \\mathbf{a}$, where $\\mathbf{a}$ is some nonzero vector in $\\mathbb{R}^m$ (refer to the class notes for notation). The momentum coefficient is kept constant at $\\mu = 0.9$ throughout training. If the risk function has a saddle point at $\\mathbf{w}_t$, what is the step $\\mathbf{v}_{t+1}$ at iteration $t$?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qTU6DAPRn1hN",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QwMq7SK4nFwU",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 2.4"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "iEaE7twinHyl",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "A friend of yours argues that in the situation described in the previous problem, the steps after iteration $t$ decay exponentially. Her argument is based on the fact that the risk is at a saddle point at $\\mathbf{w}_t$, and the momentum coefficient is constant, so that $\\mathbf{v}_{t+\\tau} = \\mu^{\\tau}\\mathbf{a}$, an exponential decay. Explain why your friend's argument is wrong."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mA05ns5Unt_6",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lfRyRIt3qCoz",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 2.5"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RWfz9zLXqFAV",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "In the situation described in Problem 2.3, will the training algorithm always eventually converge back towards $\\mathbf{w}_t$? Explain your answer briefly and clearly."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zv-spO4mqZcT",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5AMarkOTyVt3",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3: Exam-Style Questions, Set 3"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "74EwMXGkyYgz",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "The following problems take you through the computation of the set of all least-squares solutions to the following linear system:\n",
        "\n",
        "\\begin{eqnarray*}\n",
        "3x + 4y &=& 2\\\\\n",
        "3x + 4y &=& 3\n",
        "\\end{eqnarray*}\n",
        "\n",
        "and the solutions to a related optimization problem.\n",
        "All the answers to the questions in this problem are numerical and exact. They refer only to the data given in the problem, and no more general answers are required. You may leave your answers in the form of fractions, with expressions like the following:\n",
        "\n",
        "$$\n",
        "\\frac{\\sqrt{3}}{2} \\left[\\begin{array}{c} 2\\\\-5\\end{array}\\right]\\;,\n",
        "$$\n",
        "\n",
        "but please simplify as much as possible.\n",
        "\n",
        "_As usual, it is easiest to answer these questions using software (and perhaps guess the exact values from the approximate ones output by your code). However, this would rob you of the opportunity to understand this material and to practice for the exam. **In any event, no answers will be accepted to problems in this part that embed software in your submission.**_\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ac8j8SGdzDr9",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.1"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9R-zasYizH16",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "What are $A$ and $\\mathbf{b}$ if we write the system in this problem in the following form?\n",
        "$$\n",
        "A \\mathbf{x} = \\mathbf{b}\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "v6jJC2xuzKS5",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eJoEvX1bzPDh",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.2"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zA4CavTxzReO",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "What is the rank of $A$?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "aplEQX65zUHW",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "KOrmLSUVzWM6",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.3"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MUH5dslBzapt",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Give a _unit_ column vector $\\mathbf{r}$ that spans the row space of $A$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "p7vpRTgKzfEi",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DbW0YgUWzjQc",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.4"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "q6kAPv09zlWk",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Give a _unit_ column vector $\\mathbf{n}$ that spans the null space of $A$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "23yZJ6GCzo2K",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nOm2a0USzwU0",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.5"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cMNnKIARzyfL",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Write the matrix $V$ in the SVD $A = U\\Sigma V^T$ of $A$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "c7U8WqRUz0ct",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zk41d7sBz5GS",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.6"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZK3cYMIkz7Jc",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Compute the matrices $U$ and $\\Sigma$ in the SVD of $A$. [Hint: compute $U\\Sigma$ first.]"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "T3ZNlDaUz9gQ",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xHCt-FvH0FDT",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.7"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EhxCKF6U0HwH",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Compute the pseudo-inverse $A^{\\dagger}$ of $A$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "63rXWdST0KZN",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hllY12AA0PKi",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.8"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MnjLAWvt0Q06",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Find the minimum-norm least-squares solution $\\mathbf{x}^*$ of the system $A\\mathbf{x} = \\mathbf{b}$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2RgECzBj0TVw",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MDz6wROp0ZIr",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.9"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0P5mlHPz0bTA",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Give an expression for the set $S$ of all least-squares solutions of the system $A\\mathbf{x} = \\mathbf{b}$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "K2ZKErfi0dwK",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8vQg6So00ij9",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.10"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ghlx7z5s0lkG",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Find all the solutions to\n",
        "\n",
        "$$\n",
        " \\hat{\\mathbf{x}} = \\arg\\min_{\\|\\mathbf{x}\\| = 1} \\|A\\mathbf{x}\\|\\;.\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wXELtk700oFd",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8GZdQ7IsoUVR",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "## Part 4: Neural Networks"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4gQIvWQAzhzm",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "The code in this part is somewhat modified from the [Keras documentation](https://keras.io/examples/cifar10_cnn/). It downloads the CIFAR-10 dataset, a set of 60000 labeled images grouped in 10 categories, which it splits into training, validation, and test sets. It then defines a function `network` that returns a simple convolutional neural network (the `model`), and a function `train` that trains the model for a single epoch by default, checking performance on the validation set.The function `train` also saves the trained model in a file in the cloud and evaluates the model on the test data. Finally, it returns a history of training and validation accuracies achieved after each epoch of training. The function `train` uses SGD as the default optimizer.\n",
        "\n",
        "_**Important:**_ Make sure you select Python 3 through the `Runtime->Change runtime type` menu at the top of the notebook. Also set the hardware acceleration to `None` in that same menu. We will turn on GPU acceleration later on. TPU acceleration is not always available, so we won't use it."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JM7Sh_kizkO-",
        "outputId": "7dea89f6-ffa3-4d6d-a442-5dedc0f34595",
        "tags": [
          "HST"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# The data, split between train, validation, and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "(x_train, x_validate, y_train, y_validate) = train_test_split(x_train, y_train,\n",
        "                                                             test_size=0.2)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'training samples')\n",
        "print(x_validate.shape[0], 'validation samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_validate = keras.utils.to_categorical(y_validate, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_validate = x_validate.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_validate /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (40000, 32, 32, 3)\n",
            "40000 training samples\n",
            "10000 validation samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FBMRAFNN5k6S",
        "tags": [
          "HST"
        ],
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "activation_function = 'relu'\n",
        "\n",
        "def network(activation_function='relu'):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                  input_shape=x_train.shape[1:]))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "  \n",
        "  return model\n",
        "\n",
        "model = network()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "35sd9xJr55eM",
        "tags": [
          "HST"
        ],
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "from math import ceil\n",
        "\n",
        "def train(model, epochs=1,\n",
        "          opt = keras.optimizers.SGD(lr=0.01, momentum=0.7, decay=0.001),\n",
        "          verbose=2):\n",
        "\n",
        "  batch_size = 32\n",
        "  save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "  model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "  # Configure the model for training\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=opt,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(x_train, y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs,\n",
        "                      validation_data=(x_validate, y_validate),\n",
        "                      shuffle=False,\n",
        "                      verbose=verbose)\n",
        "\n",
        "  # Save model and weights\n",
        "  if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "  model_path = os.path.join(save_dir, model_name)\n",
        "  model.save(model_path)\n",
        "\n",
        "  # Score trained model.\n",
        "  scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "  print('Test loss:', scores[0])\n",
        "  print('Test accuracy:', scores[1])\n",
        "  return [history.epoch, history.history['acc'], history.history['val_acc']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JMElXg2ETjAE",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 4.1\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oDnPx1gdT5Jm",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Using Stochastic Gradient Descent (SGD) with the default parameters in `train`, train the model for one epoch _with no hardware acceleration_.\n",
        "\n",
        "Show your call to `train` and the outputs it generates. Is validation accuracy a reasonably good estimate of test accuracy?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "73TQApgKiGY_",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "#### Programming Notes\n",
        "\n",
        "+ Hardware acceleration is turned off through the `Runtime->Change runtime type` menu at the top of the notebook, and selecting `None` for hardware acceleration.\n",
        "\n",
        "+ Depending on circumstances, after you change the runtime type, some or all of the notebook state may be lost. This will require you to rerun some of the cells above.\n",
        "\n",
        "+ Tensorflow may generate warning messages that depend on how the Colaboratory interface is implemented. These messages are typically harmless."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5sYqNFyUVxdy",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "id": "deIo6eEp-6cs",
        "colab_type": "code",
        "outputId": "da7621e0-df98-411d-cf68-399cb6e7c8af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "model = network()\n",
        "[epoc_out, acc, val_acc] = train(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            " - 228s - loss: 2.0210 - acc: 0.2485 - val_loss: 1.7551 - val_acc: 0.3639\n",
            "10000/10000 [==============================] - 16s 2ms/step\n",
            "Test loss: 1.7390554656982422\n",
            "Test accuracy: 0.3723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iYaIRxEeMNK7",
        "colab_type": "code",
        "outputId": "8bcb1018-bfde-4064-bd6a-d85e3b8c5866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "epoc_out"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "cUFabarBN9By",
        "colab_type": "code",
        "outputId": "92ee6861-90ae-43d6-e026-77b7cad4888a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.248475]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "tvwjvV_COHUD",
        "colab_type": "code",
        "outputId": "c65a1e52-10dd-43e2-9594-34e6689af750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "val_acc"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3639]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "RfydNOiwO5gN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Yes. Validation accuracy is a reasonably good estimate of test accuracy in this case. You can see how the 0.3983 for test accuracy is close to the 0.3894 accuracy for validation. This assumption that validation accuracy isa . good estimate of test accuracy usually holds true--asusming that they both come from similar data. \n",
        "However, after a lot of messing with the nueral network's architecture in hopes of increasing validation accuracy, this might be less true."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NoGPF3W7WU_X",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 4.2"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yCDoSETDWXCJ",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Repeat the previous experiment _after turning on GPU acceleration_ from the `Runtime->Change runtime type` menu.\n",
        "\n",
        "Are the accuracy values the same as before? Explain why or why not. What is the approximate ratio of running times of CPU (no acceleration) versus GPU training?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JRnVqg5gXbvx",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "id": "Bjk8OzlKZMNH",
        "colab_type": "code",
        "outputId": "88b3315b-3837-4a39-8fb7-5df0c4186ad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# The data, split between train, validation, and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "(x_train, x_validate, y_train, y_validate) = train_test_split(x_train, y_train,\n",
        "                                                             test_size=0.2)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'training samples')\n",
        "print(x_validate.shape[0], 'validation samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_validate = keras.utils.to_categorical(y_validate, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_validate = x_validate.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_validate /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (40000, 32, 32, 3)\n",
            "40000 training samples\n",
            "10000 validation samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sq_K-pqMZO3W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "activation_function = 'relu'\n",
        "\n",
        "def network(activation_function='relu'):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                  input_shape=x_train.shape[1:]))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "  \n",
        "  return model\n",
        "\n",
        "model = network()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W1huPGTIZRwr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "from math import ceil\n",
        "\n",
        "def train(model, epochs=1,\n",
        "          opt = keras.optimizers.SGD(lr=0.01, momentum=0.7, decay=0.001),\n",
        "          verbose=2):\n",
        "\n",
        "  batch_size = 32\n",
        "  save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "  model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "  # Configure the model for training\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=opt,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(x_train, y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs,\n",
        "                      validation_data=(x_validate, y_validate),\n",
        "                      shuffle=False,\n",
        "                      verbose=verbose)\n",
        "\n",
        "  # Save model and weights\n",
        "  if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "  model_path = os.path.join(save_dir, model_name)\n",
        "  model.save(model_path)\n",
        "\n",
        "  # Score trained model.\n",
        "  scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "  print('Test loss:', scores[0])\n",
        "  print('Test accuracy:', scores[1])\n",
        "  return [history.epoch, history.history['acc'], history.history['val_acc']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lb59jfFtUAQS",
        "colab_type": "code",
        "outputId": "8d95bf7e-a61d-40dc-f94a-82a7ba6d2cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# model = network() ## model has already been initialized, so no need to do it again.. This line is here so the grader can see that we didn't forget to initialize\n",
        "[epoc_out, acc, val_acc] = train(model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            " - 18s - loss: 1.9992 - acc: 0.2584 - val_loss: 1.7836 - val_acc: 0.3614\n",
            "10000/10000 [==============================] - 2s 209us/step\n",
            "Test loss: 1.7679498775482179\n",
            "Test accuracy: 0.3685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BLPDkYDvWohZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70c2f7de-100b-41e3-cff1-b992de39f6e5"
      },
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.258425]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "8uKGng0NWmz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf6e4196-e2b9-4e81-8f28-b5b8846f75d9"
      },
      "cell_type": "code",
      "source": [
        "val_acc"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3614]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "7cn4kM_dUugF",
        "colab_type": "code",
        "outputId": "89975756-6c99-411a-f01c-2072e7057873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "gpu_cpu_ratio = 16 / 2\n",
        "print('The approximate ratio of running times of GPU to CPU is: ' + str(gpu_cpu_ratio))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The approximate ratio of running times of GPU to CPU is: 8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tN6bb7EEaksW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The accuracies are different in each case. The reason for this is that the initialization of the parameters in the network is randomized each time. This means the optimization for each run started at a different configuration and therefore shouldn't be expected to end up at the exact location after one epoch"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UL293H0_YerN",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 4.3"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hw67mTPzYg-4",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "We keep GPU acceleration turned on from now on.\n",
        "\n",
        "Repeat the experiment above with the ADAM optimizer with the default parameters. This optimizer selects the descent step size adaptively. The ADAM optimizer is invoked by using parameter `opt = keras.optimizers.Adam()` in `train`.\n",
        "\n",
        "Compare accuracies and running times with those achieved in the previous experiment."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PRFT6uhiZdIj",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "id": "i50p6thsMIet",
        "colab_type": "code",
        "outputId": "328780da-e37e-4b6d-9cb4-3d738735727c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "model = network()\n",
        "[epoc_out, acc, val_acc] = train(model, opt = keras.optimizers.Adam())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            " - 20s - loss: 1.6077 - acc: 0.4079 - val_loss: 1.2256 - val_acc: 0.5558\n",
            "10000/10000 [==============================] - 2s 183us/step\n",
            "Test loss: 1.2349614694595337\n",
            "Test accuracy: 0.5501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "KEp7_h36bC7R",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 4.4"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UxdkfA9ibFUM",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "We use the ADAM optimizer with default parameters from now on.\n",
        "\n",
        "Repeat the previous experiment with 30 epochs of training instead of 1 (`epochs=30`). This time, store the value returned by `train`, as you will need it for plotting.\n",
        "\n",
        "When done, plot both training accuracy and validation accuracy as functions of epoch number on the same diagram. Label the axes and add a legend to specify which plot is which.\n",
        "\n",
        "Do you think that the classifier would perform much better if you were to train longer? Explain briefly."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Pw9glyaNvwQ6",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "#### Programming Notes\n",
        "\n",
        "+ Look at the definition of `train` to figure out what the output from that function contains.\n",
        "+ Set the value of the `verbose` parameter in the call to `train` to 0 to suppress output, which would be too long to include in your PDF file. You can estimate from your previous experiments how long the code will take to run. Alternatively, set `verbose` to 2 in early test runs, but then set it to 0 in your final run."
      ]
    },
    {
      "metadata": {
        "tags": [
          "ST"
        ],
        "id": "pxOicSFohW3T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "id": "3G0Mh-2hQW0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bb180cc2-fbc6-44ff-9469-090da16ff05d"
      },
      "cell_type": "code",
      "source": [
        "model = network()\n",
        "[epoc_out, acc, val_acc] = train(model, opt = keras.optimizers.Adam(), epochs = 30, verbose = 0)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 168us/step\n",
            "Test loss: 0.716250999546051\n",
            "Test accuracy: 0.7742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mfqb3sEsPwrV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "88147713-fc7d-4ad9-a096-e4dd1e62c615"
      },
      "cell_type": "code",
      "source": [
        "# Plots for training and testing process: loss and accuracy\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "plt.figure(0)\n",
        "plt.plot(acc,'r')\n",
        "plt.plot(val_acc,'g')\n",
        "plt.xticks(np.arange(0, 31, 2.0))\n",
        "plt.rcParams['figure.figsize'] = (8, 6)\n",
        "plt.xlabel(\"Num of Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training Accuracy vs Validation Accuracy\")\n",
        "plt.legend(['train','validation'])\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAGCCAYAAADJ40tJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "dV87zWIbXBIV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I do not think this would be much better if you trained it longer. As evidenced in the graph, the accuracy measures begin to plateau--especially the validation accuracy. This is suggestive of over-fitting, which means training more will not help."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UxdW8dV4uoCq",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 4.5"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "A97WOgV8uweq",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Suggest at least three different ways to improve the performance of the classifier defined in this Part. For each way, explain why that would help. This is an open-ended question, and answers may vary. Do _not_ implement your suggestions, and do _not_ refer to techniques we have not covered in class (such as batch normalization or other techniques you may have heard of).\n",
        "\n",
        "If you suggest more than three ways, we will grade you for the best ones. However, we _will_ deduct points for patently wrong statements in any of your suggestions."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "12XxnmJwnYzN",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "id": "UZWLsXuXRleX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Adding more convolutional layers in the architecture: the more layers there are, the more parameters the system can optimize for, which will make the model more flexible. The only drawbacks might be training time and overfitting--however, since our dataset is so large, we are less exposed to overfitting.\n",
        "\n",
        "2. Change the allocation of validation and training samples. By increasing the number of training images you might be able to improve the accuracy. The drawback will be that you have a less trust-worthy validation to determine how generalizable the results are.\n",
        "\n",
        "3. Increase the number of neurons in each convolutional layer. You can do that by increasing the number of filters in each layer. This will make the model more powerful and flexible to increase the accuracy."
      ]
    }
  ]
}